{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UTK_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QitWz9_SnpKG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE38gyea9-b0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "opt = {\n",
        "    \"kernel_initializer\": \"xavier\",\n",
        "    \"bias_initializer\": \"zeros\",\n",
        "    \"batch_normalization\": True,\n",
        "    \"dropout\": True,\n",
        "    \"dropout_prob_keep\": 0.4,\n",
        "    \"depth\": 4,\n",
        "    \"filters\": [256, 512, 1024, 512],\n",
        "    \"l2_regularizer_weight\": 1e-12,\n",
        "    'batch_size': 64,\n",
        "    'lr': 1e-3,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXqs_cet-NyV"
      },
      "source": [
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters,\n",
        "                 kernel_initializer=tf.keras.initializers.GlorotNormal(),\n",
        "                 bias_initializer=tf.keras.initializers.zeros(),\n",
        "                 regularizer=None,\n",
        "                 batch_norm=False,\n",
        "                 name=''):\n",
        "        super(ResidualBlock, self).__init__(name=name)\n",
        "\n",
        "        self.conv2a = tf.keras.layers.Conv2D(filters, (1, 1),\n",
        "                                             activation=None,\n",
        "                                             use_bias=True,\n",
        "                                             kernel_initializer=kernel_initializer,\n",
        "                                             bias_initializer=bias_initializer,\n",
        "                                             kernel_regularizer=regularizer,\n",
        "                                             bias_regularizer=regularizer\n",
        "                                             )\n",
        "        self.conv2d = tf.keras.layers.Conv2D(filters, (1, 1),\n",
        "                                             activation=None,\n",
        "                                             use_bias=True,\n",
        "                                             kernel_initializer=kernel_initializer,\n",
        "                                             bias_initializer=bias_initializer,\n",
        "                                             kernel_regularizer=regularizer,\n",
        "                                             bias_regularizer=regularizer\n",
        "                                             )\n",
        "\n",
        "        self.batch_norm = batch_norm\n",
        "        if self.batch_norm:\n",
        "            self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "            self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "            self.bn2c = tf.keras.layers.BatchNormalization()\n",
        "            self.bn2d = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2b = tf.keras.layers.Conv2D(filters, (3, 3), padding='valid',\n",
        "                                             activation=None,\n",
        "                                             use_bias=True,\n",
        "                                             kernel_initializer=kernel_initializer,\n",
        "                                             bias_initializer=bias_initializer,\n",
        "                                             kernel_regularizer=regularizer,\n",
        "                                             bias_regularizer=regularizer\n",
        "                                             )\n",
        "\n",
        "        self.conv2c = tf.keras.layers.Conv2D(filters, (1, 1),\n",
        "                                             activation=None,\n",
        "                                             use_bias=True,\n",
        "                                             kernel_initializer=kernel_initializer,\n",
        "                                             bias_initializer=bias_initializer,\n",
        "                                             kernel_regularizer=regularizer,\n",
        "                                             bias_regularizer=regularizer\n",
        "                                             )\n",
        "        self.conv2d = tf.keras.layers.Conv2D(filters, (3, 3), padding='valid',\n",
        "                                             activation=None,\n",
        "                                             use_bias=True,\n",
        "                                             kernel_initializer=kernel_initializer,\n",
        "                                             bias_initializer=bias_initializer,\n",
        "                                             kernel_regularizer=regularizer,\n",
        "                                             bias_regularizer=regularizer\n",
        "                                             )\n",
        "\n",
        "    def call(self, input_tensor, training=False):\n",
        "        x = self.conv2a(input_tensor)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn2a(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2b(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn2b(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2c(x)\n",
        "        input_tensor = self.conv2d(input_tensor)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn2c(x, training=training)\n",
        "            input_tensor = self.bn2d(input_tensor, training=training)\n",
        "        x += input_tensor\n",
        "        return tf.nn.relu(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3PYNnWh-Slg"
      },
      "source": [
        "class CNNModel(tf.keras.Model):\n",
        "    def __init__(self, opt):\n",
        "        super(CNNModel, self).__init__(name='')\n",
        "        self.depth = opt['depth']\n",
        "\n",
        "        if opt['kernel_initializer'] == 'gaussian':\n",
        "            kernel_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "        elif opt['kernel_initializer'] == 'xavier' or opt['kernel_initializer'] == 'glorot':\n",
        "            kernel_initializer = tf.keras.initializers.GlorotNormal()\n",
        "        elif opt['kernel_initializer'] == 'zeros':\n",
        "            kernel_initializer = tf.keras.initializers.zeros()\n",
        "        else:\n",
        "            kernel_initializer = tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "        if opt['bias_initializer'] == 'gaussian':\n",
        "            bias_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
        "        elif opt['kernel_initializer'] == 'xavier' or opt['kernel_initializer'] == 'glorot':\n",
        "            bias_initializer = tf.keras.initializers.GlorotNormal()\n",
        "        elif opt['kernel_initializer'] == 'zeros':\n",
        "            bias_initializer = tf.keras.initializers.zeros()\n",
        "        else:\n",
        "            bias_initializer = tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "        if 'l2_regularizer_weight' in opt:\n",
        "            regularizer = tf.keras.regularizers.L2(opt['l2_regularizer_weight'])\n",
        "        else:\n",
        "            regularizer = None\n",
        "\n",
        "        self.batch_norm = opt['batch_normalization']\n",
        "        filters = opt['filters']\n",
        "        self.pooling = tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(3, 3), strides=2, padding='valid'\n",
        "        )\n",
        "        for i in range(self.depth):\n",
        "            name = \"resblock{}\".format(i + 1)\n",
        "            setattr(self, name, ResidualBlock(filters[i],\n",
        "                                              kernel_initializer=kernel_initializer,\n",
        "                                              bias_initializer=bias_initializer,\n",
        "                                              regularizer=regularizer,\n",
        "                                              batch_norm=self.batch_norm,\n",
        "                                              name=name)\n",
        "                    )\n",
        "            \"\"\"\n",
        "            name = \"conv{}\".format(i + 1)\n",
        "            setattr(self, name, tf.keras.layers.Conv2D(filters[i], (3, 3),\n",
        "                                                       activation=None,\n",
        "                                                       use_bias=True,\n",
        "                                                       kernel_initializer=kernel_initializer,\n",
        "                                                       bias_initializer=bias_initializer,\n",
        "                                                       kernel_regularizer=regularizer,\n",
        "                                                       bias_regularizer=regularizer\n",
        "                                                       )\n",
        "                    )\n",
        "            \"\"\"\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(\n",
        "            256,\n",
        "            activation=None,\n",
        "            use_bias=True,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=regularizer,\n",
        "            bias_regularizer=regularizer\n",
        "        )\n",
        "        self.fc2 = tf.keras.layers.Dense(\n",
        "            128,\n",
        "            activation=None,\n",
        "            use_bias=True,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=regularizer,\n",
        "            bias_regularizer=regularizer\n",
        "        )\n",
        "        if opt['dropout']:\n",
        "            self.dropout = tf.keras.layers.Dropout(opt['dropout_prob_keep'])\n",
        "        else:\n",
        "            self.dropout = None\n",
        "        self.reg = tf.keras.layers.Dense(\n",
        "            1,\n",
        "            activation=None,\n",
        "            use_bias=True,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=regularizer,\n",
        "            bias_regularizer=regularizer\n",
        "        )\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        for i in range(self.depth):\n",
        "            \"\"\"\n",
        "            name = \"conv{}\".format(i + 1)\n",
        "            f = getattr(self, name)\n",
        "            x = f(x)\n",
        "            \n",
        "            # TODO: We may add if condition for batch normalization\n",
        "            x = tf.nn.relu(x)\n",
        "            \"\"\"\n",
        "            name = \"resblock{}\".format(i + 1)\n",
        "            f = getattr(self, name)\n",
        "            x = f(x, training=training)\n",
        "            x = self.pooling(x)\n",
        "\n",
        "        # x = tf.reshape(x, [-1, 1 * 1 * 256])\n",
        "        x = self.flatten(x)\n",
        "        # TODO: We may add if condition for batch normalization\n",
        "        x = self.fc1(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        if self.dropout is not None:\n",
        "            x = self.dropout(x, training=training)\n",
        "        x = self.reg(x)\n",
        "        return x, tf.math.round(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II0Qiczv-ZWz"
      },
      "source": [
        "model = CNNModel(opt)\n",
        "#_ = model(tf.zeros([1, 91, 91, 3]))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=opt['lr'])\n",
        "mae_loss_fn = tf.keras.losses.MeanAbsoluteError()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddvY70PQ9Phh"
      },
      "source": [
        "# model.build([None,91,91,1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUYY9VLu9UeE"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uoZ3od_Ccxr",
        "outputId": "a23b0f6f-0e4b-4062-ece4-e821a011e2b7"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuQi3k8kD4w1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58372b4d-2ea1-4e2a-bc38-8d1d70576590"
      },
      "source": [
        "%cd /content/drive/MyDrive/CS559/Homework"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS559/Homework\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3QdBcSz-cdm"
      },
      "source": [
        "import numpy as np\n",
        "x_train = np.load(\"training_data.npy\")\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "y_train = np.load(\"training_labels.npy\")\n",
        "x_val = np.load(\"validation_data.npy\")\n",
        "x_val = np.expand_dims(x_val, axis=-1)\n",
        "\n",
        "y_val = np.load(\"validation_labels.npy\")\n",
        "x_test = np.load(\"test_data.npy\")\n",
        "x_test = np.expand_dims(x_test, axis=-1)\n",
        "y_test = np.load(\"test_labels.npy\")\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_val = x_val / 255\n",
        "x_test = x_test / 255\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-A4m3fM-gam"
      },
      "source": [
        "# train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "batch_size = opt['batch_size']\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAzfUk2Q-iiL"
      },
      "source": [
        "import time\n",
        "\n",
        "\n",
        "train_loss_metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "val_loss_metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "train_acc_metric = tf.keras.metrics.Accuracy(name=\"train_acc_metric\")\n",
        "val_acc_metric = tf.keras.metrics.Accuracy(name=\"val_acc_metric\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42PMqYTT-mvD",
        "outputId": "3343a32c-dee0-474d-cb84-6c7d19e6da9c"
      },
      "source": [
        "epochs = 50\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "val_loss_results = []\n",
        "val_accuracy_results = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.device('/gpu:0') as dev:\n",
        "            with tf.GradientTape() as tape:\n",
        "                output, pred = model(x_batch_train, training=True)\n",
        "                loss_value = mae_loss_fn(y_batch_train, output)\n",
        "            grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update training metric.\n",
        "        train_loss_metric.update_state(y_batch_train, output)\n",
        "        train_acc_metric.update_state(y_batch_train, pred)\n",
        "        \"\"\"\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "        \"\"\"\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_loss = train_loss_metric.result()\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training loss over epoch: %.4f, acc %.4f\" % (float(train_loss),float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_loss_metric.reset_states()\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_output, val_preds = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_loss_metric.update_state(y_batch_val, val_output)\n",
        "        val_acc_metric.update_state(y_batch_val, val_preds)\n",
        "    \n",
        "    val_loss = val_loss_metric.result()\n",
        "    val_loss_metric.reset_states()\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "\n",
        "    train_loss_results.append(train_loss)\n",
        "    train_accuracy_results.append(train_acc)\n",
        "    val_loss_results.append(val_loss)\n",
        "    val_accuracy_results.append(val_acc)\n",
        "\n",
        "    print(\"Validation loss over epoch: %.4f, acc: %.4f\" % (float(val_loss),float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss over epoch: 12.4669, acc 0.0263\n",
            "Validation loss over epoch: 26.9991, acc: 0.0082\n",
            "Time taken: 144.53s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss over epoch: 10.7696, acc 0.0446\n",
            "Validation loss over epoch: 25.3617, acc: 0.0095\n",
            "Time taken: 135.72s\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss over epoch: 9.8108, acc 0.0480\n",
            "Validation loss over epoch: 16.5190, acc: 0.0121\n",
            "Time taken: 135.81s\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss over epoch: 9.3647, acc 0.0570\n",
            "Validation loss over epoch: 14.6823, acc: 0.0212\n",
            "Time taken: 135.67s\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss over epoch: 8.7647, acc 0.0628\n",
            "Validation loss over epoch: 12.2538, acc: 0.0285\n",
            "Time taken: 136.36s\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss over epoch: 8.6993, acc 0.0659\n",
            "Validation loss over epoch: 11.3702, acc: 0.0587\n",
            "Time taken: 136.25s\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss over epoch: 8.0390, acc 0.0737\n",
            "Validation loss over epoch: 10.1299, acc: 0.0622\n",
            "Time taken: 136.62s\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss over epoch: 7.9957, acc 0.0759\n",
            "Validation loss over epoch: 8.1771, acc: 0.0678\n",
            "Time taken: 136.18s\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss over epoch: 7.6971, acc 0.0744\n",
            "Validation loss over epoch: 7.6977, acc: 0.0639\n",
            "Time taken: 136.19s\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss over epoch: 7.5394, acc 0.0748\n",
            "Validation loss over epoch: 9.6505, acc: 0.0799\n",
            "Time taken: 136.55s\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss over epoch: 7.0595, acc 0.0811\n",
            "Validation loss over epoch: 9.1437, acc: 0.0760\n",
            "Time taken: 135.95s\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss over epoch: 6.9932, acc 0.0757\n",
            "Validation loss over epoch: 8.5920, acc: 0.0898\n",
            "Time taken: 135.98s\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss over epoch: 7.0043, acc 0.0861\n",
            "Validation loss over epoch: 8.7110, acc: 0.0475\n",
            "Time taken: 136.11s\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss over epoch: 6.6556, acc 0.0731\n",
            "Validation loss over epoch: 7.8582, acc: 0.0631\n",
            "Time taken: 136.22s\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss over epoch: 6.5489, acc 0.0870\n",
            "Validation loss over epoch: 7.4560, acc: 0.0924\n",
            "Time taken: 136.73s\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss over epoch: 6.2819, acc 0.0906\n",
            "Validation loss over epoch: 7.7008, acc: 0.0933\n",
            "Time taken: 136.06s\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss over epoch: 6.2489, acc 0.0869\n",
            "Validation loss over epoch: 7.8318, acc: 0.0773\n",
            "Time taken: 136.63s\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss over epoch: 5.8376, acc 0.0889\n",
            "Validation loss over epoch: 7.5063, acc: 0.0920\n",
            "Time taken: 136.71s\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss over epoch: 5.9537, acc 0.0911\n",
            "Validation loss over epoch: 7.4637, acc: 0.0760\n",
            "Time taken: 136.20s\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss over epoch: 6.0012, acc 0.0874\n",
            "Validation loss over epoch: 6.9574, acc: 0.0786\n",
            "Time taken: 136.86s\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss over epoch: 5.6226, acc 0.0978\n",
            "Validation loss over epoch: 7.2803, acc: 0.0760\n",
            "Time taken: 136.18s\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss over epoch: 5.3331, acc 0.0944\n",
            "Validation loss over epoch: 6.8789, acc: 0.0886\n",
            "Time taken: 136.54s\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss over epoch: 5.4376, acc 0.0900\n",
            "Validation loss over epoch: 8.5894, acc: 0.0825\n",
            "Time taken: 136.25s\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss over epoch: 5.4269, acc 0.0996\n",
            "Validation loss over epoch: 6.5808, acc: 0.0795\n",
            "Time taken: 137.01s\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss over epoch: 5.1380, acc 0.0987\n",
            "Validation loss over epoch: 9.1037, acc: 0.0674\n",
            "Time taken: 136.33s\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss over epoch: 5.2501, acc 0.0993\n",
            "Validation loss over epoch: 6.8136, acc: 0.0657\n",
            "Time taken: 136.34s\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss over epoch: 5.0290, acc 0.0891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqY2SjDkwDJs"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
        "fig.suptitle('Training Metrics')\n",
        "\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[0].plot(train_loss_results,label=\"training\")\n",
        "axes[0].plot(val_loss_results,label=\"validation\")\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results,label=\"training\")\n",
        "axes[1].plot(val_accuracy_results,label=\"validation\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsq7pxQY17v9"
      },
      "source": [
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0ygxsNx2FWd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}